# -*- coding: utf-8 -*-
"""shot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i4JC5WpMGGAmibbuymuuZbrq-QTzKzwl
"""

!pip install datasets

!pip install openai
!pip install evaluate

!pip install rouge_score

!pip install git+https://github.com/google-research/bleurt.git

!pip install bert_score

import openai
from datasets import load_dataset
from evaluate import load

# Set up OpenAI API key
openai.api_key = ""
model_choice = "gpt-3.5-turbo"

# Load the TellMeWhy dataset
ds = load_dataset("StonyBrookNLP/tellmewhy")

# Extract the first 1000 test samples
test_data = ds['test'][:1000]

# Load evaluation metrics
bleu = load("bleu")
rouge = load("rouge")
bleurt = load("bleurt")
bertscore = load("bertscore")



def zero_shot_gpt3(narrative, question, model="gpt-3.5-turbo"):
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant for answering questions."},
                {"role": "user", "content": f"Context: {narrative}\nQuestion: {question}\nAnswer:"}
            ]
        )
        return response['choices'][0]['message']['content'].strip()
    except Exception as e:
        return f"An error occurred: {e}"

from tqdm import tqdm

predictions = []
references = []
detailed_results = []

# Wrap the loop with tqdm for a progress bar
for idx in tqdm(range(len(test_data['narrative'])), desc="Processing Examples"):
    narrative = test_data['narrative'][idx]
    question = test_data['question'][idx]
    true_answer = test_data['answer'][idx]

    # Get GPT's zero-shot answer
    zero_shot_answer = zero_shot_gpt3(narrative, question)

    # Store the results
    predictions.append(zero_shot_answer)
    references.append([true_answer])  

    detailed_results.append({
        "narrative": narrative,
        "question": question,
        "true_answer": true_answer,
        "predicted_answer": zero_shot_answer
    })

print("Sample Predictions:")
for i, result in enumerate(detailed_results[:10]):
    print(f"Example {i + 1}:")
    print(f"  Narrative: {result['narrative']}")
    print(f"  Question: {result['question']}")
    print(f"  True Answer: {result['true_answer']}")
    print(f"  Predicted Answer: {result['predicted_answer']}\n")

# Evaluate performance using metrics
bleu_score = bleu.compute(predictions=predictions, references=references)
rouge_scores = rouge.compute(predictions=predictions, references=[ref[0] for ref in references])
bleurt_scores = bleurt.compute(predictions=predictions, references=[ref[0] for ref in references])
bertscore_scores = bertscore.compute(predictions=predictions, references=[ref[0] for ref in references], lang="en")

# Display results
print("Performance Metrics:")
print(f"BLEU: {bleu_score['bleu']:.4f}")
print(f"ROUGE-L: {rouge_scores['rougeL']:.4f}")
print(f"BLEURT: {sum(bleurt_scores['scores']) / len(bleurt_scores['scores']):.4f}")
print(f"BERTScore (F1): {sum(bertscore_scores['f1']) / len(bertscore_scores['f1']):.4f}")

import random

# Pool of few-shot examples
few_shot_pool = [
    {
        "narrative": "Rudy was convinced that bottled waters all tasted the same. He went to the store and bought several popular brands. He went back home and set them all on a table. He spent several hours tasting them one by one. He came to the conclusion that they actually did taste different.",
        "question": "Why did Rudy go to the store?",
        "answer": "Rudy wanted to test the taste of bottled water."
    },
    {
        "narrative": "Sandra got a job at the zoo. She loved coming to work and seeing all of the animals. Sandra went to look at the polar bears during her lunch break. She watched them eat fish and jump in and out of the water. She took pictures and shared them with her friends.",
        "question": "Why did Sandra go to look at the polar bears?",
        "answer": "Sandra wanted to take pictures of the polar bears."
    },
    {
        "narrative": "Alex bought a new bicycle. He rode it every day to work. One day, he noticed a strange noise coming from the bike. He stopped to check the tires and found one was flat.",
        "question": "Why did Alex buy a new bicycle?",
        "answer": "Alex needed a way to commute to work."
    }
]

# Function to select random examples
def select_random_examples(pool, num_examples=2):
    return random.sample(pool, num_examples)

# Few-shot GPT function with dynamic example selection
def few_shot_gpt3(narrative, question, pool, model="gpt-3.5-turbo"):
    examples = select_random_examples(pool)
    few_shot_prompt = "\n\n".join(
        f"Context: {ex['narrative']}\nQuestion: {ex['question']}\nAnswer: {ex['answer']}"
        for ex in examples
    )
    prompt = (
        few_shot_prompt
        + f"\n\nContext: {narrative}\nQuestion: {question}\nAnswer:"
    )
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant for answering questions."},
                {"role": "user", "content": prompt}
            ]
        )
        return response['choices'][0]['message']['content'].strip()
    except Exception as e:
        return f"An error occurred: {e}"

# Collect predictions and references for few-shot
few_shot_predictions = []
few_shot_references = []
few_shot_detailed_results = []

# Run few-shot evaluation with progress tracking
for idx in tqdm(range(len(test_data['narrative'])), desc="Processing Few-Shot Examples"):
    narrative = test_data['narrative'][idx]
    question = test_data['question'][idx]
    true_answer = test_data['answer'][idx]

    random_examples = select_random_examples(few_shot_pool, num_examples=2)
    # Get GPT's few-shot answer
    few_shot_answer = few_shot_gpt3(narrative, question, random_examples)

    # Store results
    few_shot_predictions.append(few_shot_answer)
    few_shot_references.append([true_answer]) 
    few_shot_detailed_results.append({
        "narrative": narrative,
        "question": question,
        "true_answer": true_answer,
        "predicted_answer": few_shot_answer
    })

# Evaluate performance
few_shot_bleu_score = bleu.compute(predictions=few_shot_predictions, references=few_shot_references)
few_shot_rouge_scores = rouge.compute(predictions=few_shot_predictions, references=[ref[0] for ref in few_shot_references])
few_shot_bleurt_scores = bleurt.compute(predictions=few_shot_predictions, references=[ref[0] for ref in few_shot_references])
few_shot_bertscore_scores = bertscore.compute(predictions=few_shot_predictions, references=[ref[0] for ref in few_shot_references], lang="en")

# Display results
print("Few-Shot Performance Metrics:")
print(f"BLEU: {few_shot_bleu_score['bleu']:.4f}")
print(f"ROUGE-L: {few_shot_rouge_scores['rougeL']:.4f}")
print(f"BLEURT: {sum(few_shot_bleurt_scores['scores']) / len(few_shot_bleurt_scores['scores']):.4f}")
print(f"BERTScore (F1): {sum(few_shot_bertscore_scores['f1']) / len(few_shot_bertscore_scores['f1']):.4f}")
